{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0202d37e-f115-44e0-863f-8ef4c54e91f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing shared computation logic from library\n",
    "\n",
    "A concept in software engineering is \"testability\", the ability to write and execute tests against business logic.  \n",
    "Notebooks by themselves are not amenable to this approach; as it mixes data access, data processing, data ingestion, and data export in the same code unit; and it makes writing separable tests much harder. \n",
    "An approach to writing testable pyspark code, in the python eco-system is to separate certain business logic in a python library file.  This file can be separably executed and tested.  \n",
    "\n",
    "This particular example shows to separate logic block, imported as a library file, and its testability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e89a957-0787-451b-b9f0-21c61b3ccf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "print(notebook_path)\n",
    "notebook_abs_dir = os.path.dirname(notebook_path)\n",
    "print(notebook_abs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ea13da-a3d5-4a3a-9b73-6a1ab5300fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5c986cf-ef87-44dc-b719-ba3f4eeadac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Datasets from external Data Source\n",
    "\n",
    "These Datasets are currently stored locally and packaged with this repo. However, it is representive of interacting with external data interfaces, such as API (i.e. HTTPS GET) or Delta Lake (i.e. spark.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddbd1c1-b86d-4dd8-9779-3ba3ef8be60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pretend we are loading data from an API to get the Class Dataset\n",
    "import os\n",
    "from library.fetch_data_from_api import get_class_data_from_api\n",
    "\n",
    "df_class = get_class_data_from_api(spark)\n",
    "\n",
    "display(df_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bf578b-9d48-446b-a257-5199d5b9a590",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754059072624}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.fetch_data_from_api import get_score_data_from_api\n",
    "\n",
    "df_score = get_score_data_from_api(spark)\n",
    "\n",
    "display(df_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36735f9-fa0d-4944-9d53-735b8d22c619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Processing - Joining Class and Score columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f968ef4a-5820-4c24-b220-8649494fbf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.class_business_logic import inner_join_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c415148-66a2-4860-bcd3-69497b52d66d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754059077985}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = inner_join_dataframes(df_class, df_score, \"class_id\")\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f004ed0d-e368-4e7f-84f3-eec8d5fe0738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calculate Class Level Statistics\n",
    "\n",
    "I will provide 2 implementation of the same logic to provide two levels of complexity of computation\n",
    "and their corresponding test cases.  The result should be the same.  I'm giving two examples of the same logic implemented in different ways.  Both approaches have their unit tests and integration tests.\n",
    "\n",
    "To make code testable, there are a couple of important guidelines to follow: \n",
    "1. Encapsulate logic in functions and place them in a separate library file—such as those found in the   `library` folder. This allows pytest to import the logic for testing without automatically executing it.\n",
    "2. Write logic in the smallest possible functional units—each function should perform a single, well-defined task. This makes it easier to wrap tests around individual pieces of logic, enabling faster error tracing and simpler debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e29b08-8997-4c3b-b9a9-2a7532fb3ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Score Statistics with Simple Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d768f46b-3a83-4189-92cc-375955c5654c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.class_business_logic import calculate_statistics_with_sql\n",
    "\n",
    "df_result = calculate_statistics_with_sql(df_joined, columns=[\"score\"], spark=spark)\n",
    "# print(df_result)\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4277c14-1ba2-429e-a68c-4720a015a328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Score Statistics with Pandas On Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd13178-c4c2-49dd-a319-9b2cc6832f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.class_business_logic import calculate_statistics_with_pandas\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "df_result = df_joined.groupBy(\"class_id\", \"class_name\").applyInPandas(\n",
    "    lambda pdf: calculate_statistics_with_pandas(pdf, columns=[\"score\"]),\n",
    "    schema=\"class_id string, class_name string, score_stats struct<score_average:double,score_min:double,score_max:double>\",\n",
    ")\n",
    "\n",
    "# explode the score_stats struct\n",
    "df_result = df_result.select(\"class_id\", \"class_name\", \"score_stats.*\")\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b024bd-592f-42c4-ba28-605ed024a621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "result = (\n",
    "    df_sample_data.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "    .groupBy(\"pickup_date\")\n",
    "    .applyInPandas(\n",
    "        lambda pdf: calculate_statistics(pdf, columns=[\"trip_distance\", \"fare_amount\"]),\n",
    "        schema=\"pickup_date date, trip_distance_stats struct<mean:double,median:double,variance:double>, fare_amount_stats struct<mean:double,median:double,variance:double>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bee730-f112-4066-b640-d38518464597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook with separate business logic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
