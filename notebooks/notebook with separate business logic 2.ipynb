{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0202d37e-f115-44e0-863f-8ef4c54e91f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing shared computation logic from library\n",
    "\n",
    "A concept in software engineering is \"testability\", the ability to write and execute tests against business logic.  \n",
    "Notebooks by themselves are not amenable to this approach; as it mixes data access, data processing, data ingestion, and data export in the same code unit; and it makes writing separable tests much harder. \n",
    "An approach to writing testable pyspark code, in the python eco-system is to separate certain business logic in a python library file.  This file can be separably executed and tested.  \n",
    "\n",
    "This particular example shows to separate logic block, imported as a library file, and its testability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e89a957-0787-451b-b9f0-21c61b3ccf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "print(notebook_path)\n",
    "notebook_abs_dir = os.path.dirname(notebook_path)\n",
    "print(notebook_abs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ea13da-a3d5-4a3a-9b73-6a1ab5300fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5c986cf-ef87-44dc-b719-ba3f4eeadac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Datasets from external Data Source\n",
    "\n",
    "These Datasets are currently stored locally and packaged with this repo. However, it is representive of interacting with external data interfaces, such as API (i.e. HTTPS GET) or Delta Lake (i.e. spark.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddbd1c1-b86d-4dd8-9779-3ba3ef8be60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pretend we are loading data from an API to get the Class Dataset\n",
    "import os\n",
    "class_example_data_path = os.path.join(notebook_abs_dir, \"data/Class_Dataset.csv\")\n",
    "class_example_data_path = \"file:/Workspace\" + class_example_data_path\n",
    "print(class_example_data_path)\n",
    "df_class = spark.read.csv(class_example_data_path, header=True, inferSchema=True)\n",
    "display(df_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bf578b-9d48-446b-a257-5199d5b9a590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "score_example_data_path = os.path.join(notebook_abs_dir, \"data/Score_Dataset.csv\")\n",
    "score_example_data_path = \"file:/Workspace\" + score_example_data_path\n",
    "print(score_example_data_path)\n",
    "\n",
    "df_score = spark.read.csv(score_example_data_path, header=True, inferSchema=True)\n",
    "display(df_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36735f9-fa0d-4944-9d53-735b8d22c619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Processing - Joining Class and Score columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f968ef4a-5820-4c24-b220-8649494fbf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.class_business_logic import inner_join_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c415148-66a2-4860-bcd3-69497b52d66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = inner_join_dataframes(df_class, df_score, \"class_id\")\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f004ed0d-e368-4e7f-84f3-eec8d5fe0738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56816613-f2cf-4dfd-b6f8-0c6020bb9603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sql_command = \"\"\"\n",
    "SELECT * FROM samples.nyctaxi.trips \n",
    "order by tpep_pickup_datetime desc\n",
    "LIMIT 5000\n",
    "\"\"\"\n",
    "df_sample_data = spark.sql(sql_command)\n",
    "display(df_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316a431e-38fe-431b-80ae-8a460c0fc5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from library.business_logic import inner_join_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b024bd-592f-42c4-ba28-605ed024a621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "result = (\n",
    "    df_sample_data.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "    .groupBy(\"pickup_date\")\n",
    "    .applyInPandas(\n",
    "        lambda pdf: calculate_statistics(pdf, columns=['trip_distance', 'fare_amount']),\n",
    "        schema=\"pickup_date date, trip_distance_stats struct<mean:double,median:double,variance:double>, fare_amount_stats struct<mean:double,median:double,variance:double>\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bee730-f112-4066-b640-d38518464597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook with separate business logic 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
